{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! kaggle competitions download -c new-york-city-taxi-fare-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667a6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets will be saved in the 'svtp_datasets/' directory.\n",
      "\n",
      "[1/8] Downloading Concrete Data...\n",
      " -> Success: Saved concrete_slump.csv\n",
      "\n",
      "[2/8] Downloading Boston Housing Data...\n",
      " -> Success: Saved boston_housing.csv\n",
      " -> Note: The Boston Housing dataset has known ethical concerns.\n",
      "\n",
      "[3/8] Downloading Kin8nm Data...\n",
      " -> Success: Saved kin8nm.csv\n",
      "\n",
      "[4/8] Downloading Yacht Hydrodynamics Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391014/2439534041.py:63: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(url, delim_whitespace=True, header=None, names=column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Success: Saved yacht_hydrodynamics.csv\n",
      "\n",
      "[5/8] Downloading Energy Efficiency Data...\n",
      " -> Success: Saved energy_efficiency.csv\n",
      "\n",
      "[6/8] Downloading Elevators Data...\n",
      " -> Success: Saved elevators.csv\n",
      "\n",
      "[7/8] Downloading Protein Structure Data...\n",
      " -> Success: Saved protein_structure.csv\n",
      "\n",
      "[8/8] Instructions for Taxi Trip Fare Data:\n",
      " -> This dataset is from the 'New York City Taxi Fare Prediction' Kaggle competition.\n",
      " -> Due to its size, it must be downloaded using the Kaggle API.\n",
      " -> Instructions:\n",
      "    1. Install the Kaggle library: pip install kaggle\n",
      "    2. Go to your Kaggle account, 'Settings' page, and click 'Create New Token'.\n",
      "    3. Place the downloaded 'kaggle.json' file in the required location (e.g., '~/.kaggle/').\n",
      "    4. Uncomment and run the code block below in a separate script or cell.\n",
      "\n",
      "Attempting to download Taxi Fare data via Kaggle API...\n",
      "Downloading new-york-city-taxi-fare-prediction.zip to svtp_datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.56G/1.56G [00:00<00:00, 3.68GB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Success: Taxi data downloaded and extracted to 'svtp_datasets/'\n",
      "\n",
      "\n",
      "All tasks complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "import zipfile\n",
    "\n",
    "def download_and_save_datasets(output_dir='temp'):\n",
    "    \"\"\"\n",
    "    Downloads, processes, and saves the datasets used in the\n",
    "    \"Sparse Variational Student-t Processes\" paper.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Create a directory to store the datasets\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Datasets will be saved in the '{output_dir}/' directory.\")\n",
    "\n",
    "    # --- 1. Concrete Slump Test Data ---\n",
    "    try:\n",
    "        print(\"\\n[1/8] Downloading Concrete Data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\"\n",
    "        # The file is comma-separated and has a header on the first line\n",
    "        df = pd.read_csv(url)\n",
    "        # Clean up column names by removing extra spaces\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df.to_csv(os.path.join(output_dir, 'concrete_slump.csv'), index=False)\n",
    "        print(\" -> Success: Saved concrete_slump.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Concrete data. Error: {e}\")\n",
    "\n",
    "    # --- 2. Boston Housing Data ---\n",
    "    try:\n",
    "        print(\"\\n[2/8] Downloading Boston Housing Data...\")\n",
    "        # The original UCI dataset is deprecated. We fetch a processed version from OpenML.\n",
    "        boston = fetch_openml(name='boston', version=1, as_frame=True, parser='liac-arff')\n",
    "        df_boston = boston.frame\n",
    "        df_boston.to_csv(os.path.join(output_dir, 'boston_housing.csv'), index=False)\n",
    "        print(\" -> Success: Saved boston_housing.csv\")\n",
    "        print(\" -> Note: The Boston Housing dataset has known ethical concerns.\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Boston Housing data. Error: {e}\")\n",
    "\n",
    "    # --- 3. Kin8nm Data ---\n",
    "    try:\n",
    "        print(\"\\n[3/8] Downloading Kin8nm Data...\")\n",
    "        # This dataset is available on OpenML\n",
    "        kin8nm = fetch_openml(name='kin8nm', version=1, as_frame=True, parser='liac-arff')\n",
    "        df_kin8nm = kin8nm.frame\n",
    "        df_kin8nm.to_csv(os.path.join(output_dir, 'kin8nm.csv'), index=False)\n",
    "        print(\" -> Success: Saved kin8nm.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Kin8nm data. Error: {e}\")\n",
    "\n",
    "    # --- 4. Yacht Hydrodynamics Data ---\n",
    "    try:\n",
    "        print(\"\\n[4/8] Downloading Yacht Hydrodynamics Data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\"\n",
    "        # The data is space-separated and has no header\n",
    "        column_names = [\n",
    "            'longitudinal_pos', 'prismatic_coeff', 'length_displacement_ratio',\n",
    "            'beam_draught_ratio', 'length_beam_ratio', 'froude_number',\n",
    "            'residuary_resistance'\n",
    "        ]\n",
    "        df = pd.read_csv(url, delim_whitespace=True, header=None, names=column_names)\n",
    "        df.to_csv(os.path.join(output_dir, 'yacht_hydrodynamics.csv'), index=False)\n",
    "        print(\" -> Success: Saved yacht_hydrodynamics.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Yacht data. Error: {e}\")\n",
    "\n",
    "    # --- 5. Energy Efficiency Data ---\n",
    "    try:\n",
    "        print(\"\\n[5/8] Downloading Energy Efficiency Data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "        # The data is in an Excel file\n",
    "        df = pd.read_excel(url, engine='openpyxl')\n",
    "        df.to_csv(os.path.join(output_dir, 'energy_efficiency.csv'), index=False)\n",
    "        print(\" -> Success: Saved energy_efficiency.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Energy Efficiency data. Error: {e}\")\n",
    "\n",
    "    # --- 6. Elevators Data ---\n",
    "    try:\n",
    "        print(\"\\n[6/8] Downloading Elevators Data...\")\n",
    "        # This dataset is available on OpenML\n",
    "        elevators = fetch_openml(name='elevators', version=1, as_frame=True, parser='liac-arff')\n",
    "        df_elevators = elevators.frame\n",
    "        df_elevators.to_csv(os.path.join(output_dir, 'elevators.csv'), index=False)\n",
    "        print(\" -> Success: Saved elevators.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Elevators data. Error: {e}\")\n",
    "\n",
    "    # --- 7. Protein Tertiary Structure Data ---\n",
    "    try:\n",
    "        print(\"\\n[7/8] Downloading Protein Structure Data...\")\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\"\n",
    "        # The data is in a CSV file with a header\n",
    "        df = pd.read_csv(url)\n",
    "        df.to_csv(os.path.join(output_dir, 'protein_structure.csv'), index=False)\n",
    "        print(\" -> Success: Saved protein_structure.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Failed to download Protein Structure data. Error: {e}\")\n",
    "\n",
    "    # --- 8. Taxi Trip Fare Data (Kaggle) ---\n",
    "    print(\"\\n[8/8] Instructions for Taxi Trip Fare Data:\")\n",
    "    print(\" -> This dataset is from the 'New York City Taxi Fare Prediction' Kaggle competition.\")\n",
    "    print(\" -> Due to its size, it must be downloaded using the Kaggle API.\")\n",
    "    print(\" -> Instructions:\")\n",
    "    print(\"    1. Install the Kaggle library: pip install kaggle\")\n",
    "    print(\"    2. Go to your Kaggle account, 'Settings' page, and click 'Create New Token'.\")\n",
    "    print(\"    3. Place the downloaded 'kaggle.json' file in the required location (e.g., '~/.kaggle/').\")\n",
    "    print(\"    4. Uncomment and run the code block below in a separate script or cell.\")\n",
    "\n",
    "    # --- CODE TO DOWNLOAD TAXI DATA (run separately) ---\n",
    "    try:\n",
    "        import kaggle\n",
    "        print(\"\\nAttempting to download Taxi Fare data via Kaggle API...\")\n",
    "        # Authenticate with kaggle.json\n",
    "        kaggle.api.authenticate()\n",
    "        # Download the dataset files\n",
    "        kaggle.api.competition_download_files(\n",
    "            'new-york-city-taxi-fare-prediction',\n",
    "            path=output_dir,\n",
    "            quiet=False\n",
    "        )\n",
    "        # Unzip the downloaded file\n",
    "        zip_path = os.path.join(output_dir, 'new-york-city-taxi-fare-prediction.zip')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        os.remove(zip_path) # Clean up the zip file\n",
    "        print(f\" -> Success: Taxi data downloaded and extracted to '{output_dir}/'\")\n",
    "    except Exception as e:\n",
    "        print(f\" -> Kaggle API download failed. Please ensure 'kaggle.json' is set up correctly. Error: {e}\")\n",
    "\n",
    "    print(\"\\n\\nAll tasks complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_dir = \"temp\"\n",
    "    download_and_save_datasets(output_dir=dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir = 'temp'\n",
    "\n",
    "dataset_dict = {\n",
    "    \"Boston\": \"boston_housing.csv\",\n",
    "    \"Concrete\": \"concrete_slump.csv\",\n",
    "    \"Kin8nm\": \"kin8nm.csv\",\n",
    "    \"Yacht\": \"yacht_hydrodynamics.csv\",\n",
    "    \"Energy\": \"energy_efficiency.csv\",\n",
    "    \"Elevators\": \"elevators.csv\",\n",
    "    \"Protein\": \"protein_structure.csv\",\n",
    "    \"Taxi\": \"train.csv\"  # Assuming you have downloaded the taxi data manually\n",
    "}\n",
    "\n",
    "for dataset_name, filename in dataset_dict.items():\n",
    "    file_path = os.path.join(dataset_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"{dataset_name} dataset found at: {file_path}\")\n",
    "        \n",
    "        # 行数をカウント（ヘッダー含む）\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "            print(f\"{dataset_name} total lines (including header): {line_count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error counting lines in {dataset_name} dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} dataset not found. Please ensure it has been downloaded and saved correctly.\")\n",
    "        continue  # ファイルがなければ次へ\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=10)  # 最初の10行だけ読み込み\n",
    "        print(f\"name: {dataset_name}, shape: {df.shape}, columns: {list(df.columns)}\")\n",
    "        print(df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {dataset_name} dataset: {e}\")\n",
    "\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42a3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROCESSING DATASET: Concrete\n",
      "==================================================\n",
      "Directory for 'Concrete' is ready at 'dataset_xu_2024/Concrete'\n",
      "--> Attempting to load data from: 'temp/concrete_slump.csv'\n",
      "--> Successfully loaded with 103 rows.\n",
      "--> Creating 5 splits for 'Concrete'...\n",
      "--> Finished saving 5 splits for 'Concrete'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Boston\n",
      "==================================================\n",
      "Directory for 'Boston' is ready at 'dataset_xu_2024/Boston'\n",
      "--> Attempting to load data from: 'temp/boston_housing.csv'\n",
      "--> Successfully loaded with 506 rows.\n",
      "--> Creating 5 splits for 'Boston'...\n",
      "--> Finished saving 5 splits for 'Boston'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Kin8nm\n",
      "==================================================\n",
      "Directory for 'Kin8nm' is ready at 'dataset_xu_2024/Kin8nm'\n",
      "--> Attempting to load data from: 'temp/kin8nm.csv'\n",
      "--> Successfully loaded with 8192 rows.\n",
      "--> Creating 5 splits for 'Kin8nm'...\n",
      "--> Finished saving 5 splits for 'Kin8nm'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Yacht\n",
      "==================================================\n",
      "Directory for 'Yacht' is ready at 'dataset_xu_2024/Yacht'\n",
      "--> Attempting to load data from: 'temp/yacht_hydrodynamics.csv'\n",
      "--> Successfully loaded with 308 rows.\n",
      "--> Creating 5 splits for 'Yacht'...\n",
      "--> Finished saving 5 splits for 'Yacht'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Energy\n",
      "==================================================\n",
      "Directory for 'Energy' is ready at 'dataset_xu_2024/Energy'\n",
      "--> Attempting to load data from: 'temp/energy_efficiency.csv'\n",
      "--> Successfully loaded with 768 rows.\n",
      "--> Creating 5 splits for 'Energy'...\n",
      "--> Finished saving 5 splits for 'Energy'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Elevators\n",
      "==================================================\n",
      "Directory for 'Elevators' is ready at 'dataset_xu_2024/Elevators'\n",
      "--> Attempting to load data from: 'temp/elevators.csv'\n",
      "--> Successfully loaded with 16599 rows.\n",
      "--> Creating 5 splits for 'Elevators'...\n",
      "--> Finished saving 5 splits for 'Elevators'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Protein\n",
      "==================================================\n",
      "Directory for 'Protein' is ready at 'dataset_xu_2024/Protein'\n",
      "--> Attempting to load data from: 'temp/protein_structure.csv'\n",
      "--> Successfully loaded with 45730 rows.\n",
      "--> Creating 5 splits for 'Protein'...\n",
      "--> Finished saving 5 splits for 'Protein'.\n",
      "\n",
      "==================================================\n",
      "PROCESSING DATASET: Taxi\n",
      "==================================================\n",
      "Directory for 'Taxi' is ready at 'dataset_xu_2024/Taxi'\n",
      "--> Attempting to load data from: 'temp/train.csv'\n",
      "--> Successfully loaded with 55423856 rows.\n",
      "--> Creating 5 splits for 'Taxi'...\n",
      "--> Finished saving 5 splits for 'Taxi'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "def split_and_save_dataset(source_file_path, output_dir, dataset_name, target_column, n_splits=10):\n",
    "    \"\"\"\n",
    "    Loads a single dataset, performs k-fold cross-validation splitting, and saves\n",
    "    the train/test sets for each fold into a structured directory.\n",
    "\n",
    "    Args:\n",
    "        source_file_path (str): The full path to the source dataset CSV file.\n",
    "        output_dir (str): The root directory where the split data will be saved.\n",
    "        dataset_name (str): The name for the dataset's subdirectory (e.g., 'Concrete').\n",
    "        target_column (str): The name of the column to be used as the target/label (y).\n",
    "        n_splits (int): The number of folds to create.\n",
    "    \"\"\"\n",
    "    # --- 1. Create Output Directory for the specific dataset ---\n",
    "    dataset_dir = os.path.join(output_dir, dataset_name)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    print(f\"Directory for '{dataset_name}' is ready at '{dataset_dir}'\")\n",
    "\n",
    "    # --- 2. Load the Dataset ---\n",
    "    print(f\"--> Attempting to load data from: '{source_file_path}'\")\n",
    "    try:\n",
    "        df = pd.read_csv(source_file_path)\n",
    "        print(f\"--> Successfully loaded with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"--> [ERROR] File Not Found: The file '{source_file_path}' does not exist.\")\n",
    "        print(f\"--> SKIPPING '{dataset_name}'.\\n\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"--> [ERROR] Could not read the file. Reason: {e}\")\n",
    "        print(f\"--> SKIPPING '{dataset_name}'.\\n\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Clean and Separate Data ---\n",
    "    original_columns = list(df.columns)\n",
    "    df.columns = [str(col).strip().lower().replace(' ', '_').replace('(', '').replace(')', '').replace('.', '') for col in df.columns]\n",
    "    standardized_target = target_column.strip().lower().replace(' ', '_').replace('(', '').replace(')', '').replace('.', '')\n",
    "\n",
    "    if standardized_target not in df.columns:\n",
    "        print(f\"--> [ERROR] Target column '{target_column}' (standardized to '{standardized_target}') was not found.\")\n",
    "        print(f\"--> Original columns were: {original_columns}\")\n",
    "        print(f\"--> Standardized columns are: {list(df.columns)}\")\n",
    "        print(f\"--> SKIPPING '{dataset_name}'.\\n\")\n",
    "        return\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "    X = df.drop(columns=[standardized_target])\n",
    "    y = df[standardized_target]\n",
    "\n",
    "    # --- 4. Set up and Perform K-Fold Cross-Validation ---\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    print(f\"--> Creating {n_splits} splits for '{dataset_name}'...\")\n",
    "    fold_number = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        split_dir = os.path.join(dataset_dir, f'split_{fold_number}')\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_train.to_csv(os.path.join(split_dir, 'train_features.csv'), index=False)\n",
    "        y_train.to_csv(os.path.join(split_dir, 'train_target.csv'), index=False, header=True)\n",
    "        X_test.to_csv(os.path.join(split_dir, 'test_features.csv'), index=False)\n",
    "        y_test.to_csv(os.path.join(split_dir, 'test_target.csv'), index=False, header=True)\n",
    "        \n",
    "        fold_number += 1\n",
    "    print(f\"--> Finished saving {n_splits} splits for '{dataset_name}'.\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    # The paper used 5-fold cross-validation. We are using 10 splits\n",
    "    # to match your requested directory structure.\n",
    "    num_splits = 5\n",
    "    \n",
    "    # Set the directory where your source CSV files are located.\n",
    "    source_data_dir = 'temp'  \n",
    "    \n",
    "    # Set the directory where you want to save the final structured dataset.\n",
    "    output_data_dir = 'dataset_xu_2024'\n",
    "\n",
    "    # Dictionary mapping dataset names to their filenames and target columns.\n",
    "    datasets_to_process = {\n",
    "        'Concrete': ('concrete_slump.csv', 'SLUMP(cm)'),\n",
    "        'Boston': ('boston_housing.csv', 'MEDV'),\n",
    "        'Kin8nm': ('kin8nm.csv', 'y'),\n",
    "        'Yacht': ('yacht_hydrodynamics.csv', 'residuary_resistance'),\n",
    "        'Energy': ('energy_efficiency.csv', 'Y1'),\n",
    "        'Elevators': ('elevators.csv', 'Goal'),\n",
    "        'Protein': ('protein_structure.csv', 'RMSD'),\n",
    "        'Taxi': ('train.csv', 'fare_amount') \n",
    "    }\n",
    "    \n",
    "    # --- Main Loop ---\n",
    "    for name, (filename, target) in datasets_to_process.items():\n",
    "        print(\"=\"*50)\n",
    "        print(f\"PROCESSING DATASET: {name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Construct the full path to the source file for each dataset\n",
    "        full_source_path = os.path.join(source_data_dir, filename)\n",
    "        \n",
    "        split_and_save_dataset(\n",
    "            source_file_path=full_source_path,\n",
    "            output_dir=output_data_dir,\n",
    "            dataset_name=name,\n",
    "            target_column=target,\n",
    "            n_splits=num_splits\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4136da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston dataset found at: svtp_datasets/boston_housing.csv\n",
      "Boston total lines (including header): 507\n",
      "name: Boston, shape: (10, 14), columns: ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n",
      "\n",
      "\n",
      "Concrete dataset found at: svtp_datasets/concrete_slump.csv\n",
      "Concrete total lines (including header): 104\n",
      "name: Concrete, shape: (10, 11), columns: ['No', 'Cement', 'Slag', 'Fly ash', 'Water', 'SP', 'Coarse Aggr.', 'Fine Aggr.', 'SLUMP(cm)', 'FLOW(cm)', 'Compressive Strength (28-day)(Mpa)']\n",
      "   No  Cement   Slag  Fly ash  Water    SP  Coarse Aggr.  Fine Aggr.  \\\n",
      "0   1   273.0   82.0    105.0  210.0   9.0         904.0       680.0   \n",
      "1   2   163.0  149.0    191.0  180.0  12.0         843.0       746.0   \n",
      "2   3   162.0  148.0    191.0  179.0  16.0         840.0       743.0   \n",
      "3   4   162.0  148.0    190.0  179.0  19.0         838.0       741.0   \n",
      "4   5   154.0  112.0    144.0  220.0  10.0         923.0       658.0   \n",
      "\n",
      "   SLUMP(cm)  FLOW(cm)  Compressive Strength (28-day)(Mpa)  \n",
      "0       23.0      62.0                               34.99  \n",
      "1        0.0      20.0                               41.14  \n",
      "2        1.0      20.0                               41.81  \n",
      "3        3.0      21.5                               42.08  \n",
      "4       20.0      64.0                               26.82  \n",
      "\n",
      "\n",
      "Kin8nm dataset found at: svtp_datasets/kin8nm.csv\n",
      "Kin8nm total lines (including header): 8193\n",
      "name: Kin8nm, shape: (10, 9), columns: ['theta1', 'theta2', 'theta3', 'theta4', 'theta5', 'theta6', 'theta7', 'theta8', 'y']\n",
      "     theta1    theta2    theta3    theta4    theta5    theta6    theta7  \\\n",
      "0 -0.015119  0.360741  0.469398  1.309675  0.988024 -0.025493  0.664071   \n",
      "1  0.360478 -0.301395  0.629183 -1.440146 -0.741637 -1.196749 -1.038444   \n",
      "2  1.563238 -1.294753  0.078987  1.432937  1.149136 -1.292140  1.562988   \n",
      "3  0.199485  0.901157 -1.356304 -0.080525 -0.976628  0.829894 -0.855649   \n",
      "4  0.659737  0.120552 -0.008756  0.648839  0.626832 -0.646539  1.318074   \n",
      "\n",
      "     theta8         y  \n",
      "0  0.062763  0.536524  \n",
      "1 -0.717461  0.308014  \n",
      "2 -0.937731  0.518900  \n",
      "3  0.930630  0.494151  \n",
      "4 -0.899172  0.470218  \n",
      "\n",
      "\n",
      "Yacht dataset found at: svtp_datasets/yacht_hydrodynamics.csv\n",
      "Yacht total lines (including header): 309\n",
      "name: Yacht, shape: (10, 7), columns: ['longitudinal_pos', 'prismatic_coeff', 'length_displacement_ratio', 'beam_draught_ratio', 'length_beam_ratio', 'froude_number', 'residuary_resistance']\n",
      "   longitudinal_pos  prismatic_coeff  length_displacement_ratio  \\\n",
      "0              -2.3            0.568                       4.78   \n",
      "1              -2.3            0.568                       4.78   \n",
      "2              -2.3            0.568                       4.78   \n",
      "3              -2.3            0.568                       4.78   \n",
      "4              -2.3            0.568                       4.78   \n",
      "\n",
      "   beam_draught_ratio  length_beam_ratio  froude_number  residuary_resistance  \n",
      "0                3.99               3.17          0.125                  0.11  \n",
      "1                3.99               3.17          0.150                  0.27  \n",
      "2                3.99               3.17          0.175                  0.47  \n",
      "3                3.99               3.17          0.200                  0.78  \n",
      "4                3.99               3.17          0.225                  1.18  \n",
      "\n",
      "\n",
      "Energy dataset found at: svtp_datasets/energy_efficiency.csv\n",
      "Energy total lines (including header): 769\n",
      "name: Energy, shape: (10, 10), columns: ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y1', 'Y2']\n",
      "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n",
      "\n",
      "\n",
      "Elevators dataset found at: svtp_datasets/elevators.csv\n",
      "Elevators total lines (including header): 16600\n",
      "name: Elevators, shape: (10, 19), columns: ['climbRate', 'Sgz', 'p', 'q', 'curRoll', 'absRoll', 'diffClb', 'diffRollRate', 'diffDiffClb', 'SaTime1', 'SaTime2', 'SaTime3', 'SaTime4', 'diffSaTime1', 'diffSaTime2', 'diffSaTime3', 'diffSaTime4', 'Sa', 'Goal']\n",
      "   climbRate   Sgz     p     q  curRoll  absRoll  diffClb  diffRollRate  \\\n",
      "0      118.0 -55.0 -0.28 -0.08     -0.2    -11.0     11.0         0.005   \n",
      "1      390.0 -45.0 -0.06 -0.07     -0.6    -12.0     11.0         0.010   \n",
      "2       68.0   6.0  0.11  0.15      0.6    -10.0     -9.0        -0.003   \n",
      "3     -358.0 -12.0 -0.20  0.13     -0.3    -11.0     -7.0         0.001   \n",
      "4     -411.0 -19.0 -0.18  0.02     -0.5    -11.0     -3.0         0.002   \n",
      "\n",
      "   diffDiffClb  SaTime1  SaTime2  SaTime3  SaTime4  diffSaTime1  diffSaTime2  \\\n",
      "0         -0.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000          0.0   \n",
      "1         -0.2  -0.0008  -0.0008  -0.0008  -0.0008       0.0000          0.0   \n",
      "2         -0.2  -0.0011  -0.0010  -0.0010  -0.0010      -0.0002          0.0   \n",
      "3         -0.1  -0.0010  -0.0010  -0.0010  -0.0010       0.0000          0.0   \n",
      "4          1.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000          0.0   \n",
      "\n",
      "   diffSaTime3  diffSaTime4      Sa   Goal  \n",
      "0          0.0          0.0 -0.0010  0.031  \n",
      "1          0.0          0.0 -0.0008  0.034  \n",
      "2          0.0          0.0 -0.0010  0.033  \n",
      "3          0.0          0.0 -0.0010  0.032  \n",
      "4          0.0          0.0 -0.0010  0.030  \n",
      "\n",
      "\n",
      "Protein dataset found at: svtp_datasets/protein_structure.csv\n",
      "Protein total lines (including header): 45731\n",
      "name: Protein, shape: (10, 10), columns: ['RMSD', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9']\n",
      "     RMSD        F1       F2       F3        F4            F5        F6  \\\n",
      "0  17.284  13558.30  4305.35  0.31754  162.1730  1.872791e+06  215.3590   \n",
      "1   6.021   6191.96  1623.16  0.26213   53.3894  8.034467e+05   87.2024   \n",
      "2   9.275   7725.98  1726.28  0.22343   67.2887  1.075648e+06   81.7913   \n",
      "3  15.851   8424.58  2368.25  0.28111   67.8325  1.210472e+06  109.4390   \n",
      "4   7.962   7460.84  1736.94  0.23280   52.4123  1.021020e+06   94.5234   \n",
      "\n",
      "        F7   F8       F9  \n",
      "0  4287.87  102  27.0302  \n",
      "1  3328.91   39  38.5468  \n",
      "2  2981.04   29  38.8119  \n",
      "3  3248.22   70  39.0651  \n",
      "4  2814.42   41  39.9147  \n",
      "\n",
      "\n",
      "Taxi dataset found at: svtp_datasets/train.csv\n",
      "Taxi total lines (including header): 55423857\n",
      "name: Taxi, shape: (10, 8), columns: ['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
      "                             key  fare_amount          pickup_datetime  \\\n",
      "0    2009-06-15 17:26:21.0000001          4.5  2009-06-15 17:26:21 UTC   \n",
      "1    2010-01-05 16:52:16.0000002         16.9  2010-01-05 16:52:16 UTC   \n",
      "2   2011-08-18 00:35:00.00000049          5.7  2011-08-18 00:35:00 UTC   \n",
      "3    2012-04-21 04:30:42.0000001          7.7  2012-04-21 04:30:42 UTC   \n",
      "4  2010-03-09 07:51:00.000000135          5.3  2010-03-09 07:51:00 UTC   \n",
      "\n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0        -73.844311        40.721319         -73.841610         40.712278   \n",
      "1        -74.016048        40.711303         -73.979268         40.782004   \n",
      "2        -73.982738        40.761270         -73.991242         40.750562   \n",
      "3        -73.987130        40.733143         -73.991567         40.758092   \n",
      "4        -73.968095        40.768008         -73.956655         40.783762   \n",
      "\n",
      "   passenger_count  \n",
      "0                1  \n",
      "1                1  \n",
      "2                2  \n",
      "3                1  \n",
      "4                1  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263013d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f66e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e3c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

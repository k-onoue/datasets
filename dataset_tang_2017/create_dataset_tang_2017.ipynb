{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b5100f",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- understand Neal, KEEL, UCI\n",
    "- create 10-fold train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc11b076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_list = list(range(10))\n",
    "seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6ab7c",
   "metadata": {},
   "source": [
    "# Neal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dfd8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "class NealDatasetGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate and manage synthetic datasets based on the papers \n",
    "    by Neal (1997) and Tang et al. (2017).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_train_samples: int = 100,\n",
    "                 n_test_samples: int = 50,\n",
    "                 target_outlier_fraction: float = 0.05,\n",
    "                 input_outlier_fraction: float = 0.05,\n",
    "                 seed: int = None):\n",
    "        \"\"\"\n",
    "        Initializes the generator.\n",
    "\n",
    "        Args:\n",
    "            n_train_samples (int): The number of samples for the training data.\n",
    "            n_test_samples (int): The number of samples for the test data.\n",
    "            target_outlier_fraction (float): The fraction of outliers in the target variable.\n",
    "            input_outlier_fraction (float): The fraction of outliers in the input variable.\n",
    "            seed (int, optional): A random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_train_samples = n_train_samples\n",
    "        self.n_test_samples = n_test_samples\n",
    "        self.target_outlier_fraction = target_outlier_fraction\n",
    "        self.input_outlier_fraction = input_outlier_fraction\n",
    "        self.seed = seed\n",
    "        print(\"NealDatasetGenerator initialized.\")\n",
    "\n",
    "    def _generate_data(self, \n",
    "                       n_samples: int,\n",
    "                       target_outlier_fraction: float,\n",
    "                       input_outlier_fraction: float,\n",
    "                       random_seed: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Internal method to generate a single dataset.\"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "\n",
    "        # 1. Generate true inputs from the underlying distribution\n",
    "        x_true = np.random.randn(n_samples)\n",
    "        \n",
    "        # This will become the observed input, potentially with outliers\n",
    "        x_observed = x_true.copy()\n",
    "\n",
    "        # 2. Determine target outliers first to ensure reproducibility\n",
    "        target_outlier_mask = np.random.rand(n_samples) < target_outlier_fraction\n",
    "        \n",
    "        # 3. Add outliers to the input variable if specified\n",
    "        input_outlier_mask = np.zeros(n_samples, dtype=bool)\n",
    "        if input_outlier_fraction > 0.0:\n",
    "            num_input_outliers = int(n_samples * input_outlier_fraction)\n",
    "            outlier_indices = np.random.choice(n_samples, num_input_outliers, replace=False)\n",
    "            outlier_magnitude = 3 * np.std(x_true)\n",
    "            signs = np.random.choice([-1, 1], num_input_outliers)\n",
    "            x_observed[outlier_indices] += signs * outlier_magnitude\n",
    "            input_outlier_mask[outlier_indices] = True\n",
    "\n",
    "        # 4. Calculate the true mean using the (potentially outlier) observed inputs\n",
    "        true_mean = 0.3 + 0.4 * x_observed + 0.5 * np.sin(2.7 * x_observed) + 1.1 / (1 + x_observed**2)\n",
    "        \n",
    "        # 5. Generate observation noise for the target\n",
    "        noise_std_normal, noise_std_outlier = 0.1, 1.0\n",
    "        noise_std = np.where(target_outlier_mask, noise_std_outlier, noise_std_normal)\n",
    "        noise = np.random.normal(0, noise_std)\n",
    "        \n",
    "        # 6. Calculate the final observed target\n",
    "        t = true_mean + noise\n",
    "\n",
    "        # 7. Sort the data based on the observed input for easier plotting\n",
    "        sort_indices = np.argsort(x_observed)\n",
    "        \n",
    "        return {\n",
    "            \"x\": x_observed[sort_indices],\n",
    "            \"t\": t[sort_indices],\n",
    "            \"input_mask\": input_outlier_mask[sort_indices],\n",
    "            \"target_mask\": target_outlier_mask[sort_indices]\n",
    "        }\n",
    "\n",
    "    def make_train_dataset(self, with_input_outliers: bool = False) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates one type of training dataset.\n",
    "\n",
    "        Args:\n",
    "            with_input_outliers (bool): If True, the dataset will contain input outliers.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing the generated training data with keys \n",
    "                  'x', 't', 'input_mask', and 'target_mask'.\n",
    "        \"\"\"\n",
    "        input_fraction = self.input_outlier_fraction if with_input_outliers else 0.0\n",
    "        print(f\"Generating Training Data (with_input_outliers={with_input_outliers})...\")\n",
    "        \n",
    "        return self._generate_data(\n",
    "            n_samples=self.n_train_samples,\n",
    "            target_outlier_fraction=self.target_outlier_fraction,\n",
    "            input_outlier_fraction=input_fraction,\n",
    "            random_seed=self.seed\n",
    "        )\n",
    "\n",
    "    def make_test_dataset(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates the test dataset. The test data contains no outliers.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing the generated test data with keys \n",
    "                  'x', 't', 'input_mask', and 'target_mask'.\n",
    "        \"\"\"\n",
    "        print(\"Generating Test Data...\")\n",
    "        # Use a different seed for the test set to ensure it's independent from the training set\n",
    "        test_seed = None if self.seed is None else self.seed + 10\n",
    "\n",
    "        return self._generate_data(\n",
    "            n_samples=self.n_test_samples,\n",
    "            target_outlier_fraction=0.0,\n",
    "            input_outlier_fraction=0.0,\n",
    "            random_seed=test_seed\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_dataset(data: Dict[str, np.ndarray], title: str):\n",
    "        \"\"\"\n",
    "        Visualizes a single dataset.\n",
    "\n",
    "        Args:\n",
    "            data (Dict): A dictionary in the format returned by _generate_data.\n",
    "            title (str): The title for the plot.\n",
    "        \"\"\"\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        x_data, t_data = data[\"x\"], data[\"t\"]\n",
    "        input_mask, target_mask = data[\"input_mask\"], data[\"target_mask\"]\n",
    "        \n",
    "        # Create masks for different data point types\n",
    "        normal_mask = ~input_mask & ~target_mask\n",
    "        target_only_mask = ~input_mask & target_mask\n",
    "        input_only_mask = input_mask & ~target_mask\n",
    "        both_mask = input_mask & target_mask\n",
    "\n",
    "        # Plot each type of data point with a different style\n",
    "        ax.scatter(x_data[normal_mask], t_data[normal_mask], c='black', s=30, label='Normal Data', alpha=0.7)\n",
    "        ax.scatter(x_data[target_only_mask], t_data[target_only_mask], facecolors='none', edgecolors='blue', marker='o', s=100, linewidth=1.5, label='Target Outlier')\n",
    "        ax.scatter(x_data[input_only_mask], t_data[input_only_mask], c='green', marker='P', s=70, label='Input Outlier', alpha=0.8)\n",
    "        ax.scatter(x_data[both_mask], t_data[both_mask], c='red', marker='*', s=150, label='Input & Target Outlier', alpha=0.8)\n",
    "\n",
    "        # Plot the true underlying function\n",
    "        x_range = np.linspace(x_data.min() - 0.2, x_data.max() + 0.2, 400)\n",
    "        true_y = 0.3 + 0.4 * x_range + 0.5 * np.sin(2.7 * x_range) + 1.1 / (1 + x_range**2)\n",
    "        ax.plot(x_range, true_y, 'r--', label='True Function', linewidth=2)\n",
    "\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        ax.set_xlabel('Input (x)', fontsize=12)\n",
    "        ax.set_ylabel('Target (t)', fontsize=12)\n",
    "        ax.legend(fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Example Usage ---\n",
    "    \n",
    "    # 1. Create an instance of the generator\n",
    "    generator = NealDatasetGenerator(seed=2024)\n",
    "\n",
    "    # 2. Generate required datasets individually\n",
    "    \n",
    "    # Training data (target outliers only)\n",
    "    train_set_1 = generator.make_train_dataset(with_input_outliers=False)\n",
    "    \n",
    "    # Training data (both input and target outliers)\n",
    "    train_set_2 = generator.make_train_dataset(with_input_outliers=True)\n",
    "\n",
    "    # Test data\n",
    "    test_set = generator.make_test_dataset()\n",
    "\n",
    "    # # 3. Check the information of the generated data\n",
    "    # print(\"\\n--- Data Generation Summary ---\")\n",
    "    # print(f\"Train set 1 (target outliers only): x shape {train_set_1['x'].shape}, {np.sum(train_set_1['target_mask'])} target outliers.\")\n",
    "    # print(f\"Train set 2 (all outliers): x shape {train_set_2['x'].shape}, {np.sum(train_set_2['input_mask'])} input outliers.\")\n",
    "    # print(f\"Test set (no outliers): x shape {test_set['x'].shape}\\n\")\n",
    "    \n",
    "    # # 4. Plot specific datasets for verification\n",
    "    # NealDatasetGenerator.plot_dataset(train_set_1, \"Training Data (Target Outliers Only)\")\n",
    "    # NealDatasetGenerator.plot_dataset(train_set_2, \"Training Data (Input & Target Outliers)\")\n",
    "    # NealDatasetGenerator.plot_dataset(test_set, \"Test Data (No Outliers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb1e45b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_0\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_1\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_2\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_3\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_4\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_5\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_6\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_7\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_8\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=False)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal/split_9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_path_base = f\"./../datasets/Neal\"\n",
    "\n",
    "for seed in seed_list:\n",
    "\n",
    "    save_path = os.path.join(save_path_base, f\"split_{seed}\")\n",
    "\n",
    "    generator = NealDatasetGenerator(seed=seed)\n",
    "    \n",
    "    # Generate datasets\n",
    "    train_set = generator.make_train_dataset(with_input_outliers=False)\n",
    "    test_set = generator.make_test_dataset()\n",
    "    \n",
    "    train_x = train_set['x'].reshape(-1, 1)\n",
    "    train_t = train_set['t'].reshape(-1, 1)\n",
    "    test_x = test_set['x'].reshape(-1, 1)\n",
    "    test_t = test_set['t'].reshape(-1, 1)\n",
    "\n",
    "    # Save datasets as csv\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_x, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_t, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_x, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_t, delimiter=\",\")\n",
    "    print(f\"Data saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "441daa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_0\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_1\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_2\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_3\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_4\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_5\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_6\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_7\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_8\n",
      "NealDatasetGenerator initialized.\n",
      "Generating Training Data (with_input_outliers=True)...\n",
      "Generating Test Data...\n",
      "Data saved to ./../datasets/Neal_XOutlier/split_9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_path_base = f\"./../datasets/Neal_XOutlier\"\n",
    "\n",
    "for seed in seed_list:\n",
    "\n",
    "    save_path = os.path.join(save_path_base, f\"split_{seed}\")\n",
    "\n",
    "    generator = NealDatasetGenerator(seed=seed)\n",
    "    \n",
    "    # Generate datasets\n",
    "    train_set = generator.make_train_dataset(with_input_outliers=True)\n",
    "    test_set = generator.make_test_dataset()\n",
    "    \n",
    "    train_x = train_set['x'].reshape(-1, 1)\n",
    "    train_t = train_set['t'].reshape(-1, 1)\n",
    "    test_x = test_set['x'].reshape(-1, 1)\n",
    "    test_t = test_set['t'].reshape(-1, 1)\n",
    "\n",
    "    # Save datasets as csv\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_x, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_t, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_x, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_t, delimiter=\",\")\n",
    "    print(f\"Data saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.210831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.552990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.370028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.980796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.726283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.895889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.950775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.240893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.269755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.620470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  -3.210831\n",
       "1  -2.552990\n",
       "2  -2.370028\n",
       "3  -1.980796\n",
       "4  -1.726283\n",
       "..       ...\n",
       "95  1.895889\n",
       "96  1.950775\n",
       "97  2.240893\n",
       "98  2.269755\n",
       "99  2.620470\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp = os.path.join(save_path_base, f\"split_{seed_list[0]}\", \"train_features.csv\")\n",
    "df = pd.read_csv(temp, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd30e3",
   "metadata": {},
   "source": [
    "# KEEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "013c324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation:\n",
      "\n",
      "Fold 1:\n",
      "./../datasets/Diabetes/split_0\n",
      "Fold 2:\n",
      "./../datasets/Diabetes/split_1\n",
      "Fold 3:\n",
      "./../datasets/Diabetes/split_2\n",
      "Fold 4:\n",
      "./../datasets/Diabetes/split_3\n",
      "Fold 5:\n",
      "./../datasets/Diabetes/split_4\n",
      "Fold 6:\n",
      "./../datasets/Diabetes/split_5\n",
      "Fold 7:\n",
      "./../datasets/Diabetes/split_6\n",
      "Fold 8:\n",
      "./../datasets/Diabetes/split_7\n",
      "Fold 9:\n",
      "./../datasets/Diabetes/split_8\n",
      "Fold 10:\n",
      "./../datasets/Diabetes/split_9\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./../data/diabetes.dat\"\n",
    "save_path_base = f\"./../datasets/Diabetes\"\n",
    "\n",
    "# 1. ファイルから全ての行をリストとして読み込む\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "filtered_lines = [\n",
    "    line for line in lines \n",
    "    if not line.strip().startswith((\"@\"))\n",
    "]\n",
    "\n",
    "# 1. リストを単一の文字列に結合\n",
    "data_as_string = \"\".join(filtered_lines)\n",
    "\n",
    "# 2. 文字列をファイルのように扱えるように変換\n",
    "string_io = io.StringIO(data_as_string)\n",
    "\n",
    "# 3. pandas.read_csvで読み込み、DataFrameを作成\n",
    "# header=None は、ファイルにヘッダー行がないことを示す\n",
    "df = pd.read_csv(string_io, header=None)\n",
    "\n",
    "# 4. (推奨) DataFrameに列名を設定する\n",
    "df.columns = ['Age', 'Deficit', 'C_peptide']\n",
    "\n",
    "\n",
    "# 特徴量とターゲットを分離\n",
    "X = df[['Age', 'Deficit']].values\n",
    "y = df['C_peptide'].values\n",
    "\n",
    "# 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True と random_state を指定して再現性を確保\n",
    "\n",
    "# 各分割でのデータを格納するリスト (オプション)\n",
    "# train_features_list = []\n",
    "# train_target_list = []\n",
    "# test_features_list = []\n",
    "# test_target_list = []\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X[train_index]\n",
    "    train_target = y[train_index]\n",
    "    test_features = X[test_index]\n",
    "    test_target = y[test_index]\n",
    "\n",
    "    # csv として保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(save_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a143825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation:\n",
      "\n",
      "Fold 1:\n",
      "./../datasets/Machine CPU/split_0\n",
      "Fold 2:\n",
      "./../datasets/Machine CPU/split_1\n",
      "Fold 3:\n",
      "./../datasets/Machine CPU/split_2\n",
      "Fold 4:\n",
      "./../datasets/Machine CPU/split_3\n",
      "Fold 5:\n",
      "./../datasets/Machine CPU/split_4\n",
      "Fold 6:\n",
      "./../datasets/Machine CPU/split_5\n",
      "Fold 7:\n",
      "./../datasets/Machine CPU/split_6\n",
      "Fold 8:\n",
      "./../datasets/Machine CPU/split_7\n",
      "Fold 9:\n",
      "./../datasets/Machine CPU/split_8\n",
      "Fold 10:\n",
      "./../datasets/Machine CPU/split_9\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./../data/machineCPU.dat\"\n",
    "save_path_base = f\"./../datasets/Machine CPU\"\n",
    "\n",
    "# 1. ファイルから全ての行をリストとして読み込む\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "filtered_lines = [\n",
    "    line for line in lines \n",
    "    if not line.strip().startswith((\"@\"))\n",
    "]\n",
    "\n",
    "# 1. リストを単一の文字列に結合\n",
    "data_as_string = \"\".join(filtered_lines)\n",
    "\n",
    "# 2. 文字列をファイルのように扱えるように変換\n",
    "string_io = io.StringIO(data_as_string)\n",
    "\n",
    "# 3. pandas.read_csvで読み込み、DataFrameを作成\n",
    "# header=None は、ファイルにヘッダー行がないことを示す\n",
    "df = pd.read_csv(string_io, header=None)\n",
    "\n",
    "# 4. (推奨) DataFrameに列名を設定する\n",
    "df.columns = [\"MYCT\", \"MMIN\", \"MMAX\", \"CACH\", \"CHMIN\", \"CHMAX\", \"PRP\"]\n",
    "\n",
    "\n",
    "# 特徴量とターゲットを分離\n",
    "X = df[[\"MYCT\", \"MMIN\", \"MMAX\", \"CACH\", \"CHMIN\", \"CHMAX\"]].values\n",
    "y = df[\"PRP\"].values\n",
    "\n",
    "# 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True と random_state を指定して再現性を確保\n",
    "\n",
    "# 各分割でのデータを格納するリスト (オプション)\n",
    "# train_features_list = []\n",
    "# train_target_list = []\n",
    "# test_features_list = []\n",
    "# test_target_list = []\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X[train_index]\n",
    "    train_target = y[train_index]\n",
    "    test_features = X[test_index]\n",
    "    test_target = y[test_index]\n",
    "\n",
    "    # csv として保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(save_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54ac06ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation:\n",
      "\n",
      "Fold 1:\n",
      "./../datasets/ELE/split_0\n",
      "Fold 2:\n",
      "./../datasets/ELE/split_1\n",
      "Fold 3:\n",
      "./../datasets/ELE/split_2\n",
      "Fold 4:\n",
      "./../datasets/ELE/split_3\n",
      "Fold 5:\n",
      "./../datasets/ELE/split_4\n",
      "Fold 6:\n",
      "./../datasets/ELE/split_5\n",
      "Fold 7:\n",
      "./../datasets/ELE/split_6\n",
      "Fold 8:\n",
      "./../datasets/ELE/split_7\n",
      "Fold 9:\n",
      "./../datasets/ELE/split_8\n",
      "Fold 10:\n",
      "./../datasets/ELE/split_9\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./../data/ele-1.dat\"\n",
    "save_path_base = f\"./../datasets/ELE\"\n",
    "\n",
    "# 1. ファイルから全ての行をリストとして読み込む\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "filtered_lines = [\n",
    "    line for line in lines \n",
    "    if not line.strip().startswith((\"@\"))\n",
    "]\n",
    "\n",
    "# 1. リストを単一の文字列に結合\n",
    "data_as_string = \"\".join(filtered_lines)\n",
    "\n",
    "# 2. 文字列をファイルのように扱えるように変換\n",
    "string_io = io.StringIO(data_as_string)\n",
    "\n",
    "# 3. pandas.read_csvで読み込み、DataFrameを作成\n",
    "# header=None は、ファイルにヘッダー行がないことを示す\n",
    "df = pd.read_csv(string_io, header=None)\n",
    "\n",
    "# 4. (推奨) DataFrameに列名を設定する\n",
    "df.columns = [\"Inhabitants\", \"Distance\", \"Length\"]\n",
    "\n",
    "\n",
    "# 特徴量とターゲットを分離\n",
    "X = df[[\"Inhabitants\", \"Distance\"]].values\n",
    "y = df[\"Length\"].values\n",
    "\n",
    "# 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True と random_state を指定して再現性を確保\n",
    "\n",
    "# 各分割でのデータを格納するリスト (オプション)\n",
    "# train_features_list = []\n",
    "# train_target_list = []\n",
    "# test_features_list = []\n",
    "# test_target_list = []\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X[train_index]\n",
    "    train_target = y[train_index]\n",
    "    test_features = X[test_index]\n",
    "    test_target = y[test_index]\n",
    "\n",
    "    # csv として保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(save_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e10d573",
   "metadata": {},
   "source": [
    "# UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d3baf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation for MPG dataset:\n",
      "\n",
      "Fold 1:\n",
      "(352, 7) (352, 1) (40, 7) (40, 1)\n",
      "Saving to ./../datasets/MPG/split_0\n",
      "Fold 2:\n",
      "(352, 7) (352, 1) (40, 7) (40, 1)\n",
      "Saving to ./../datasets/MPG/split_1\n",
      "Fold 3:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_2\n",
      "Fold 4:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_3\n",
      "Fold 5:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_4\n",
      "Fold 6:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_5\n",
      "Fold 7:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_6\n",
      "Fold 8:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_7\n",
      "Fold 9:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_8\n",
      "Fold 10:\n",
      "(353, 7) (353, 1) (39, 7) (39, 1)\n",
      "Saving to ./../datasets/MPG/split_9\n",
      "\n",
      "MPG dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. データセットの取得\n",
    "auto_mpg = fetch_ucirepo(id=9)\n",
    "X_df = auto_mpg.data.features\n",
    "y_df = auto_mpg.data.targets # これは 'mpg' 列を含むDataFrameです\n",
    "\n",
    "# 2. 'horsepower' 列の欠損値処理\n",
    "# 'horsepower' が欠損値でない行のマスクを作成\n",
    "not_nan_mask = X_df['horsepower'].notna()\n",
    "\n",
    "# マスクを特徴量とターゲットの両方のDataFrameに適用\n",
    "X_clean_df = X_df[not_nan_mask]\n",
    "y_clean_df = y_df[not_nan_mask]\n",
    "\n",
    "# 3. NumPy配列への変換\n",
    "X_np = X_clean_df.values\n",
    "y_np = y_clean_df.values\n",
    "\n",
    "# 4. 保存先の設定\n",
    "save_path_base = f\"./../datasets/MPG\" # データセット名をMPGに変更\n",
    "\n",
    "# 5. 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # 再現性のためにrandom_stateを設定\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation for MPG dataset:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_np, y_np)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X_np[train_index]\n",
    "    train_target = y_np[train_index]\n",
    "    test_features = X_np[test_index]\n",
    "    test_target = y_np[test_index]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # CSVとして保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(f\"Saving to {save_path}\")\n",
    "    os.makedirs(save_path, exist_ok=True) # split_{fold} ディレクトリを作成\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")\n",
    "\n",
    "print(\"\\nMPG dataset processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation for MPG dataset:\n",
      "\n",
      "Fold 1:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_0\n",
      "Fold 2:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_1\n",
      "Fold 3:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_2\n",
      "Fold 4:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_3\n",
      "Fold 5:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_4\n",
      "Fold 6:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_5\n",
      "Fold 7:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_6\n",
      "Fold 8:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_7\n",
      "Fold 9:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_8\n",
      "Fold 10:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_9\n",
      "\n",
      "MPG dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. データセットの取得\n",
    "concrete = fetch_ucirepo(id=165)\n",
    "X_np = concrete.data.features.values\n",
    "y_np = concrete.data.targets.values\n",
    "\n",
    "# 4. 保存先の設定\n",
    "save_path_base = f\"./../datasets/Concrete\" # データセット名をMPGに変更\n",
    "\n",
    "# 5. 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # 再現性のためにrandom_stateを設定\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation for MPG dataset:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_np, y_np)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X_np[train_index]\n",
    "    train_target = y_np[train_index]\n",
    "    test_features = X_np[test_index]\n",
    "    test_target = y_np[test_index]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # 更にここから，訓練データは 320 インスタンス，テストデータは 80 インスタンスのみ抽出する\n",
    "    train_features = train_features[:320]\n",
    "    train_target = train_target[:320]\n",
    "    test_features = test_features[:80]\n",
    "    test_target = test_target[:80]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # CSVとして保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(f\"Saving to {save_path}\")\n",
    "    os.makedirs(save_path, exist_ok=True) # split_{fold} ディレクトリを作成\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")\n",
    "\n",
    "print(\"\\nMPG dataset processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59aa9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation for Concrete dataset:\n",
      "\n",
      "Fold 1:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_0\n",
      "Fold 2:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_1\n",
      "Fold 3:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_2\n",
      "Fold 4:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_3\n",
      "Fold 5:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_4\n",
      "Fold 6:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_5\n",
      "Fold 7:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_6\n",
      "Fold 8:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_7\n",
      "Fold 9:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_8\n",
      "Fold 10:\n",
      "(927, 8) (927, 1) (103, 8) (103, 1)\n",
      "(320, 8) (320, 1) (80, 8) (80, 1)\n",
      "Saving to ./../datasets/Concrete/split_9\n",
      "\n",
      "Concrete dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. データセットの取得\n",
    "concrete = fetch_ucirepo(id=165)\n",
    "X_np = concrete.data.features.values\n",
    "y_np = concrete.data.targets.values\n",
    "\n",
    "# 4. 保存先の設定\n",
    "save_path_base = f\"./../datasets/Concrete\" # データセット名をMPGに変更\n",
    "\n",
    "# 5. 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # 再現性のためにrandom_stateを設定\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation for Concrete dataset:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_np, y_np)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X_np[train_index]\n",
    "    train_target = y_np[train_index]\n",
    "    test_features = X_np[test_index]\n",
    "    test_target = y_np[test_index]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # 更にここから，訓練データは 320 インスタンス，テストデータは 80 インスタンスのみ抽出する\n",
    "    train_features = train_features[:320]\n",
    "    train_target = train_target[:320]\n",
    "    test_features = test_features[:80]\n",
    "    test_target = test_target[:80]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # CSVとして保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(f\"Saving to {save_path}\")\n",
    "    os.makedirs(save_path, exist_ok=True) # split_{fold} ディレクトリを作成\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")\n",
    "\n",
    "print(\"\\nConcrete dataset processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5af886cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross-validation for Bike dataset:\n",
      "\n",
      "Fold 1:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_0\n",
      "Fold 2:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_1\n",
      "Fold 3:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_2\n",
      "Fold 4:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_3\n",
      "Fold 5:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_4\n",
      "Fold 6:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_5\n",
      "Fold 7:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_6\n",
      "Fold 8:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_7\n",
      "Fold 9:\n",
      "(15641, 16) (15641, 1) (1738, 16) (1738, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_8\n",
      "Fold 10:\n",
      "(15642, 16) (15642, 1) (1737, 16) (1737, 1)\n",
      "(320, 16) (320, 1) (80, 16) (80, 1)\n",
      "Saving to ./../datasets/Bike/split_9\n",
      "\n",
      "Bike dataset processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# 1. データセットの取得\n",
    "# Capital Bike Sharing (hourly＋daily rental counts)\n",
    "bike_sharing = fetch_ucirepo(id=275)\n",
    "\n",
    "features_selected = bike_sharing.data.original.drop(columns=[\"dteday\"])\n",
    "target = bike_sharing.data.original[\"cnt\"]\n",
    "\n",
    "X_np = features_selected.values\n",
    "y_np = target.values.reshape(-1, 1)  # ターゲットを2次元配列に変換\n",
    "\n",
    "# 4. 保存先の設定\n",
    "save_path_base = f\"./../datasets/Bike\" # データセット名をMPGに変更\n",
    "\n",
    "# 5. 10分割交差検証の設定\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # 再現性のためにrandom_stateを設定\n",
    "\n",
    "print(f\"Performing {n_splits}-fold cross-validation for Bike dataset:\\n\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_np, y_np)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "\n",
    "    train_features = X_np[train_index]\n",
    "    train_target = y_np[train_index]\n",
    "    test_features = X_np[test_index]\n",
    "    test_target = y_np[test_index]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # 更にここから，訓練データは 320 インスタンス，テストデータは 80 インスタンスのみ抽出する\n",
    "    train_features = train_features[:320]\n",
    "    train_target = train_target[:320]\n",
    "    test_features = test_features[:80]\n",
    "    test_target = test_target[:80]\n",
    "\n",
    "    print(train_features.shape, train_target.shape, test_features.shape, test_target.shape)\n",
    "\n",
    "    # CSVとして保存\n",
    "    save_path = os.path.join(save_path_base, f\"split_{fold}\")\n",
    "\n",
    "    print(f\"Saving to {save_path}\")\n",
    "    os.makedirs(save_path, exist_ok=True) # split_{fold} ディレクトリを作成\n",
    "    np.savetxt(os.path.join(save_path, \"train_features.csv\"), train_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"train_target.csv\"), train_target, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_features.csv\"), test_features, delimiter=\",\")\n",
    "    np.savetxt(os.path.join(save_path, \"test_target.csv\"), test_target, delimiter=\",\")\n",
    "\n",
    "print(\"\\nBike dataset processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09519a1e",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neal', 'Bike', 'Concrete', 'Machine CPU', 'ELE', 'Diabetes', 'MPG', 'Neal_XOutlier']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "(320, 16) (320,) (80, 16) (80,)\n"
     ]
    }
   ],
   "source": [
    "# Dataset loader module for CSV splits\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "\n",
    "    def __init__(self, base_path):\n",
    "        \"\"\"\n",
    "        Initializes the DatasetLoader with the base path where datasets are stored.\n",
    "        \n",
    "        Parameters:\n",
    "            base_path (str): Root path where datasets/ lives.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def list_datasets(self):\n",
    "        \"\"\"\n",
    "        Returns a list of dataset names under the base_path directory.\n",
    "        \"\"\"\n",
    "        return [name for name in os.listdir(self.base_path)\n",
    "                if os.path.isdir(os.path.join(self.base_path, name))]\n",
    "\n",
    "    def list_splits(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Returns a sorted list of split indices available for the given dataset.\n",
    "        \"\"\"\n",
    "        ds_path = os.path.join(self.base_path, dataset_name)\n",
    "        splits = []\n",
    "        for name in os.listdir(ds_path):\n",
    "            if name.startswith(\"split_\"):\n",
    "                try:\n",
    "                    idx = int(name.split(\"_\", 1)[1])\n",
    "                    splits.append(idx)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return sorted(splits)\n",
    "\n",
    "    def load_split(self, dataset_name, split_index):\n",
    "        \"\"\"\n",
    "        Loads train/test features and targets for a specific split.\n",
    "        \n",
    "        Parameters:\n",
    "            base_path (str): Root path where datasets/ lives.\n",
    "            dataset_name (str): Name of the dataset folder.\n",
    "            split_index (int): Split number (e.g., 0, 1, ...).\n",
    "        \n",
    "        Returns:\n",
    "            X_train (ndarray), y_train (ndarray), X_test (ndarray), y_test (ndarray)\n",
    "        \"\"\"\n",
    "        split_dir = os.path.join(self.base_path, dataset_name, f\"split_{split_index}\")\n",
    "        X_train = np.loadtxt(os.path.join(split_dir, \"train_features.csv\"), delimiter=\",\")\n",
    "        y_train = np.loadtxt(os.path.join(split_dir, \"train_target.csv\"),  delimiter=\",\")\n",
    "        X_test  = np.loadtxt(os.path.join(split_dir, \"test_features.csv\"),  delimiter=\",\")\n",
    "        y_test  = np.loadtxt(os.path.join(split_dir, \"test_target.csv\"),   delimiter=\",\")\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "base = \"./../datasets\"\n",
    "\n",
    "loader = DatasetLoader(base)\n",
    "\n",
    "# 利用可能なデータセット\n",
    "print(loader.list_datasets())\n",
    "# -> ['Bike', 'Concrete', 'Diabetes', ...]\n",
    "\n",
    "# 'Bike' データセットの利用可能な分割\n",
    "print(loader.list_splits(\"Bike\"))\n",
    "# -> [0, 1, 2, ..., 9]\n",
    "\n",
    "# split 0 をロード\n",
    "X_tr, y_tr, X_te, y_te = loader.load_split(\"Bike\", 0)\n",
    "print(X_tr.shape, y_tr.shape, X_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e9305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-qep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
